# -*- coding: utf-8 -*-
"""Regression Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Yp_0ZX-kkq1fJQ-YzrbT0OAUQDw5ANK
"""

pip install --upgrade scikit-learn

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler                              #importing all necessary libraries and modules

"""### **Loading and Preprocessing**"""

from sklearn.datasets import fetch_california_housing
cal = fetch_california_housing(as_frame=True)
df = cal.frame
df.head(n=5) #structure of data could be understood

df.info()    #function info give size of data,number of rows and columns,datatypes,missing values(data contain no missing values)

df.describe() #describe give idea about values in the data,looking from mean and std we could identidy outliers and remove it from data for better efficency

"""AveRooms and AveBedrms have extreme values → Applying capping to avoid skewed predictions"""

#AveRooms
Q1 = df['AveRooms'].quantile(0.25)
Q3 = df['AveRooms'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
df['AveRooms'] = np.clip(df['AveRooms'], lower_bound, upper_bound)  # Cap outlier

#AveBedrms
Q1 = df['AveBedrms'].quantile(0.25)
Q3 = df['AveBedrms'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
df['AveBedrms'] = np.clip(df['AveBedrms'], lower_bound, upper_bound)  # Cap outlier

df.describe()

"""Feature Scaling
Some models (e.g., SVR, Gradient Boosting) perform better when features have similar scales.
"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df.drop(columns=["MedHouseVal"]))# Scaling all features except target

df["RoomsPerHousehold"] = df["AveRooms"] / df["AveOccup"]
df["BedrmsPerRoom"] = df["AveBedrms"] / df["AveRooms"]

df

X = df.drop(columns=["MedHouseVal"])
y = df["MedHouseVal"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""1. Linear Regression                                                                                                                                            
 Linear Regression finds the best-fit line that minimizes the error between the predicted and actual values. It assumes a linear relationship between the independent variables and the target variable.

2. Decision Tree Regressor                                                                                                                         
  A Decision Tree splits the dataset into smaller subsets by asking yes/no questions at each node, creating a tree-like structure. It predicts by following the branches to a final leaf node.                     
3. Random Forest Regressor                                                                                                            
 Random Forest is an ensemble method that builds multiple Decision Trees on different random subsets of data and averages their predictions, making it more stable and robust.                                      
4. Gradient Boosting Regressor                                                                                                        
 Gradient Boosting is another ensemble method that builds trees sequentially, with each tree learning from the errors of the previous one, improving predictions over time.                                            
5. Support Vector Regressor (SVR)                                                                                                         
SVR tries to fit the data within a margin of tolerance (epsilon). It uses kernel tricks to transform data into a higher dimension where a linear fit is more effective.
"""

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Initialize models
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42),
    "Support Vector Regressor": SVR()
}

# Train and evaluate models
results = {}
for name, model in models.items():
    if name == "Support Vector Regressor":
        model.fit(X_train_scaled, y_train)  # Train with scaled features
        y_pred = model.predict(X_test_scaled)  # Predict with scaled features
    else:
        model.fit(X_train, y_train)  # Train on original features
        y_pred = model.predict(X_test)  # Predict on original features

    # Calculate performance metrics
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    results[name] = {"MSE": mse, "MAE": mae, "R²": r2}

# Convert results to DataFrame for easy comparison
results_df = pd.DataFrame(results).T
print(results_df)

"""**Random Forest Regressor has the lowest MSE (0.256658) and MAE (0.329710), and the highest R² (0.804139). This indicates that Random Forest is capturing the variability in housing prices the best, making it the top-performing model.**

**On the other hand, Decision Tree Regressor shows the highest MSE (0.531560), the lowest R² (0.594355), and its MAE is also relatively higher compared to some models. This suggests that while Decision Trees are good at capturing non-linear relationships, on its own it may overfit or fail to generalize as well as the ensemble methods in this scenario.**

"""

